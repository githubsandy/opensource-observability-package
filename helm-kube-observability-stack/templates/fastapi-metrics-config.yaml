apiVersion: v1
kind: ConfigMap
metadata:
  name: fastapi-metrics-config
  namespace: {{ .Values.namespace }}
data:
  main.py: |
    """
    Sample FastAPI Application with Prometheus Metrics
    
    This demonstrates how to instrument your FastAPI application
    for monitoring test automation and backend services.
    """
    from fastapi import FastAPI, HTTPException
    from fastapi.responses import JSONResponse
    from prometheus_client import Counter, Histogram, Gauge, generate_latest
    from prometheus_client import multiprocess, CollectorRegistry
    import time
    import asyncio
    import logging
    import os
    
    # Configure logging
    logging.basicConfig(level=getattr(logging, os.getenv("LOG_LEVEL", "INFO")))
    logger = logging.getLogger(__name__)
    
    app = FastAPI(
        title="Test Automation FastAPI Metrics",
        description="Sample FastAPI app with Prometheus metrics for test automation platforms",
        version="1.0.0"
    )
    
    # Prometheus Metrics for Test Automation Platform
    
    # Test Execution Metrics
    test_executions_total = Counter(
        'test_executions_total', 
        'Total number of test executions',
        ['framework', 'status', 'environment']
    )
    
    test_execution_duration = Histogram(
        'test_execution_duration_seconds',
        'Time spent executing tests',
        ['framework', 'test_type'],
        buckets=[1, 5, 10, 30, 60, 300, 600, 1800]  # Test execution time buckets
    )
    
    # CXTAF/CXTM Framework Metrics
    cxtaf_device_connections = Gauge(
        'cxtaf_device_connections_active',
        'Number of active device connections in CXTAF'
    )
    
    cxtm_workflows_active = Gauge(
        'cxtm_workflows_active',
        'Number of active test workflows in CXTM'
    )
    
    # API Performance Metrics
    api_request_duration = Histogram(
        'fastapi_request_duration_seconds',
        'FastAPI request duration',
        ['method', 'endpoint', 'status_code'],
        buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0]
    )
    
    api_requests_total = Counter(
        'fastapi_requests_total',
        'Total FastAPI requests',
        ['method', 'endpoint', 'status_code']
    )
    
    # Test Result Metrics
    test_results_stored = Counter(
        'test_results_stored_total',
        'Number of test results stored in database',
        ['result_type', 'database']
    )
    
    # Business Logic Metrics
    active_test_sessions = Gauge(
        'active_test_sessions_total',
        'Number of concurrent test sessions'
    )
    
    # Health Check Endpoints
    @app.get("/health")
    async def health():
        return {"status": "healthy", "timestamp": time.time()}
    
    @app.get("/ready") 
    async def ready():
        return {"status": "ready", "timestamp": time.time()}
    
    # Sample Test Execution API
    @app.post("/api/test/execute")
    async def execute_test(test_request: dict):
        start_time = time.time()
        framework = test_request.get("framework", "unknown")
        environment = test_request.get("environment", "unknown")
        
        try:
            # Simulate test execution
            await asyncio.sleep(0.1)  # Simulate processing time
            
            # Update metrics
            test_executions_total.labels(
                framework=framework, 
                status="success", 
                environment=environment
            ).inc()
            
            test_execution_duration.labels(
                framework=framework,
                test_type="api"
            ).observe(time.time() - start_time)
            
            api_requests_total.labels(
                method="POST",
                endpoint="/api/test/execute", 
                status_code="200"
            ).inc()
            
            return {"status": "success", "test_id": "test_123", "duration": time.time() - start_time}
            
        except Exception as e:
            test_executions_total.labels(
                framework=framework,
                status="failed",
                environment=environment
            ).inc()
            
            api_requests_total.labels(
                method="POST", 
                endpoint="/api/test/execute",
                status_code="500"
            ).inc()
            
            raise HTTPException(status_code=500, detail=str(e))
    
    # CXTAF Device Management API
    @app.post("/api/cxtaf/connect")
    async def connect_device():
        cxtaf_device_connections.inc()
        active_test_sessions.inc()
        return {"status": "connected", "device_id": "device_001"}
    
    @app.post("/api/cxtaf/disconnect")
    async def disconnect_device():
        cxtaf_device_connections.dec()
        active_test_sessions.dec() 
        return {"status": "disconnected"}
    
    # CXTM Workflow Management API
    @app.post("/api/cxtm/workflow/start")
    async def start_workflow():
        cxtm_workflows_active.inc()
        return {"status": "started", "workflow_id": "wf_001"}
    
    @app.post("/api/cxtm/workflow/stop")
    async def stop_workflow():
        cxtm_workflows_active.dec()
        return {"status": "stopped"}
    
    # Test Results Storage API
    @app.post("/api/results/store")
    async def store_results(results: dict):
        database = results.get("database", "mongodb")
        result_type = results.get("type", "test_result")
        
        test_results_stored.labels(
            result_type=result_type,
            database=database
        ).inc()
        
        return {"status": "stored", "count": 1}
    
    # Sample metrics endpoint (for development/testing)
    @app.get("/sample-metrics")
    async def sample_metrics():
        """Generate sample metrics for testing"""
        # Simulate some test activity
        test_executions_total.labels(framework="cxtaf", status="success", environment="staging").inc()
        test_executions_total.labels(framework="cxtm", status="success", environment="production").inc()
        
        cxtaf_device_connections.set(5)
        cxtm_workflows_active.set(3)
        active_test_sessions.set(8)
        
        return {"message": "Sample metrics generated"}
  
  requirements.txt: |
    fastapi==0.104.1
    uvicorn==0.24.0
    prometheus-client==0.19.0
    pydantic==2.5.0
  
  Dockerfile: |
    FROM python:3.11-slim
    
    WORKDIR /app
    
    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt
    
    COPY main.py .
    
    # Expose both app and metrics ports
    EXPOSE 8000 8001
    
    # Start FastAPI with metrics endpoint
    CMD ["python", "-c", "
    import uvicorn
    import os
    from main import app
    from prometheus_client import start_http_server, generate_latest
    from threading import Thread
    
    # Start Prometheus metrics server on port 8001
    def start_metrics_server():
        start_http_server(int(os.getenv('METRICS_PORT', 8001)))
        
    metrics_thread = Thread(target=start_metrics_server)
    metrics_thread.start()
    
    # Start FastAPI app on port 8000
    uvicorn.run(app, host='0.0.0.0', port=int(os.getenv('APP_PORT', 8000)))
    "]